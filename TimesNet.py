import os, random, warnings, time, math
warnings.filterwarnings("ignore")
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score

import tensorflow as tf
from tensorflow.keras import layers, models, callbacks, optimizers, metrics, regularizers

# ========= éš¨æ©Ÿç¨®å­ =========
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE); random.seed(RANDOM_STATE); tf.random.set_seed(RANDOM_STATE)

# ========= è‡ªå‹•è¨­å®šæ”¯æ´ä¸­æ–‡çš„å­—å‹ =========
font_paths = [r"C:\Windows\Fonts\msjh.ttc",
              "/System/Library/Fonts/PingFang.ttc",
              "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc"]
for fp in font_paths:
    if os.path.exists(fp):
        try:
            prop = fm.FontProperties(fname=fp)
            plt.rcParams["font.family"] = prop.get_name()
            break
        except Exception:
            pass
plt.rcParams["axes.unicode_minus"] = False

# ======= 4 å€‹è³‡æ–™é›†è·¯å¾‘ï¼ˆèˆ‡ä½ ä¹‹å‰ç›¸åŒï¼‰=======
DATASETS = [
    ("xg01", r"c:\Users\admin\Desktop\å­¸å§Š\è³‡æ–™\2024-03-19_sm00_UDP_Bandlock_9S_Phone_#01_All.csv"),
    ("xg02", r"c:\Users\admin\Desktop\å­¸å§Š\è³‡æ–™\2024-03-19_sm00_UDP_Bandlock_9S_Phone_#02_All.csv"),
    ("xg03", r"c:\Users\admin\Desktop\å­¸å§Š\è³‡æ–™\2024-03-19_sm01_UDP_Bandlock_9S_Phone_#01_All.csv"),
    ("xg04", r"c:\Users\admin\Desktop\å­¸å§Š\è³‡æ–™\2024-03-19_sm01_UDP_Bandlock_9S_Phone_#02_All.csv"),
]

# æ™‚é–“çª—ï¼ˆä»¥ã€Œæ­¥æ•¸ã€ï¼›ä½ çš„è³‡æ–™ 10Hz â†’ 10æ­¥=1ç§’ï¼‰
WINDOW_SIZES   = [10, 20, 30]
FUTURE_WINDOWS = [10, 20, 30]
TEST_SIZE      = 0.3

# è¦–è¦ºåŒ–ç´°ç¯€ï¼ˆèˆ‡ CNN/LSTM/PatchTST åŒæ­¥ï¼‰
SHOW_ROW_SUM = True
STAT_FONT    = 16
STAT_LINE_H  = 0.18

# ======= åŸºæœ¬å·¥å…· =======
def ensure_cols(df, required_cols):
    assert all(c in df.columns for c in required_cols), \
        f"CSV ç¼ºå°‘å¿…è¦æ¬„ä½ï¼Œéœ€åŒ…å«ï¼š{required_cols}ï¼Œç›®å‰æ¬„ä½ï¼š{df.columns.tolist()}"

def build_xy_seq(df, ws, fw):
    """å›å‚³åºåˆ— X: (N, ws, 2) åŠ y: (N,)ï¼›ç‰¹å¾µ= [RSRP, RSRQ]"""
    X, y = [], []
    rsrp_all = df["RSRP"].values
    rsrq_all = df["RSRQ"].values
    rlf_all  = df["RLF_II"].values
    for start_idx in range(0, len(df) - ws - fw + 1):
        seq_rsrp = rsrp_all[start_idx:start_idx+ws]
        seq_rsrq = rsrq_all[start_idx:start_idx+ws]
        future   = rlf_all[start_idx+ws:start_idx+ws+fw]
        X.append(np.stack([seq_rsrp, rsrq_all[start_idx:start_idx+ws]], axis=-1))  # (ws, 2)
        y.append(1 if (future != 0).any() else 0)
    return np.array(X, np.float32), np.array(y, np.int32)

def standardize_by_train(X_train, X_val):
    feat_dim = X_train.shape[-1]
    flat = X_train.reshape(-1, feat_dim)
    mean = flat.mean(axis=0); std = flat.std(axis=0) + 1e-8
    return (X_train - mean)/std, (X_val - mean)/std

def best_threshold(y_val, y_prob, start=0.1, end=0.91, step=0.01):
    best_t, best_f1 = 0.5, -1.0; t = start
    while t <= end + 1e-9:
        f1 = f1_score(y_val, (y_prob >= t).astype(int), zero_division=0)
        if f1 > best_f1: best_f1, best_t = f1, t
        t += step
    return best_t, best_f1

# ======= äº‹ä»¶åœ–å·¥å…·ï¼ˆèˆ‡ä½ å…¶ä»–æ¨¡å‹å®Œå…¨ä¸€è‡´ï¼‰=======
def has_consecutive(arr: np.ndarray, run_len: int) -> bool:
    if run_len <= 1: return arr.sum() >= 1
    cnt = 0
    for v in arr:
        cnt = cnt + 1 if v == 1 else 0
        if cnt >= run_len: return True
    return False

def compute_stats(det_matrix, num_events_total):
    n_events_eval = det_matrix.shape[0]
    if n_events_eval == 0: return dict(at1=0, c2=0, c3=0, a2=0, a3=0)
    sums = det_matrix.sum(axis=1)
    return dict(
        at1=int((sums >= 1).sum()),
        a2=int((sums >= 2).sum()),
        a3=int((sums >= 3).sum()),
        c2=int(sum(has_consecutive(det_matrix[i], 2) for i in range(n_events_eval))),
        c3=int(sum(has_consecutive(det_matrix[i], 3) for i in range(n_events_eval))),
    )

def build_event_matrices(df, WINDOW_SIZE, FUTURE_WINDOW, val_idx, y_pred_val):
    event_indices = np.where(df["RLF_II"].values != 0)[0]
    num_events_total = len(event_indices)
    val_set = set(val_idx.tolist())
    events_for_eval = []
    for j in event_indices:
        low  = max(0, j - WINDOW_SIZE - FUTURE_WINDOW + 1)
        high = min(j - WINDOW_SIZE, len(df) - WINDOW_SIZE - FUTURE_WINDOW)
        if high >= low:
            windows = list(range(low, high + 1))
            win_in_val = [s for s in windows if s in val_set]
            pos_in_val = []
            for s in win_in_val:
                pos = np.where(val_idx == s)[0]
                if pos.size > 0 and y_pred_val[pos[0]] == 1:
                    pos_in_val.append(s)
            events_for_eval.append((j, win_in_val, pos_in_val))
        else:
            events_for_eval.append((j, [], []))
    n_events_eval = len(events_for_eval)
    det_matrix = np.zeros((n_events_eval, WINDOW_SIZE), dtype=int)
    for i, (j, win_in_val, pos_in_val) in enumerate(events_for_eval):
        if not win_in_val: continue
        low = max(0, j - WINDOW_SIZE - FUTURE_WINDOW + 1)
        for s in pos_in_val:
            k = s - low
            if 0 <= k < WINDOW_SIZE: det_matrix[i, k] = 1
    return det_matrix, num_events_total, n_events_eval

def plot_and_save(det_matrix, ws, fw, num_events_total, out_png_path):
    n_events = det_matrix.shape[0]
    fig = plt.figure(figsize=(11, 7.5), dpi=150)
    ax_main = fig.add_axes([0.08, 0.12, 0.62, 0.78])
    ax_stat = fig.add_axes([0.73, 0.20, 0.25, 0.60]); ax_stat.axis('off')

    if n_events == 0:
        ax_main.set_title("No evaluable events (no valid windows in validation set)")
        fig.savefig(out_png_path, bbox_inches='tight'); plt.close(fig); return

    stats = compute_stats(det_matrix, num_events_total)
    at1, c2, c3, a2, a3 = stats['at1'], stats['c2'], stats['c3'], stats['a2'], stats['a3']

    for i in range(n_events):
        for k in range(ws):
            hit = det_matrix[i, k] == 1
            ax_main.scatter(k + 1, i + 1, s=60,
                            facecolors=('red' if hit else 'white'),
                            edgecolors=('red' if hit else 'black'),
                            linewidths=0.8, alpha=1.0, antialiased=False)
    if SHOW_ROW_SUM:
        counts = det_matrix.sum(axis=1)
        for i, c in enumerate(counts): ax_main.text(ws + 1.2, i + 1, str(c), va='center', fontsize=9)

    ax_main.set_xlabel("Window index before event (1 oldest ... N just before)")
    ax_main.set_ylabel("Event # (actual occurrences)")
    ax_main.set_xlim(0.5, ws + 5.0); ax_main.set_ylim(0.5, n_events + 1); ax_main.invert_yaxis()

    labels = ["è‡³å°‘1é»ç‚ºç´…","é€£çºŒ2é»ç‚ºç´…","é€£çºŒ3é»ç‚ºç´…","ä»»æ„2é»ç‚ºç´…","ä»»æ„3é»ç‚ºç´…"]
    nums   = [at1, c2, c3, a2, a3]
    y, dy, fs = 1.0, STAT_LINE_H, STAT_FONT
    for lab, num in zip(labels, nums):
        ax_stat.text(0.00, y, lab, ha='left', va='top', fontsize=fs)
        ax_stat.text(0.66, y, f": {num}/{num_events_total} = {num/num_events_total*100:.1f}%",
                     ha='left', va='top', fontsize=fs)
        y -= dy

    fig.patch.set_facecolor('white'); ax_main.set_facecolor('white')
    fig.savefig(out_png_path, bbox_inches='tight'); plt.close(fig)

def save_learning_curve(history, out_png_path, title="TimesNet Training and Validation AUC"):
    auc_tr  = history.history.get("auc", [])
    auc_va  = history.history.get("val_auc", [])
    x_axis = range(1, len(auc_tr)+1)
    plt.figure(figsize=(8,5), dpi=150)
    plt.plot(x_axis, auc_tr, label='Train AUC')
    plt.plot(x_axis, auc_va, linestyle='--', label='Validation AUC')  # Val è™›ç·š
    plt.xlabel('Epochs'); plt.ylabel('AUC'); plt.title(title); plt.legend(); plt.tight_layout()
    plt.savefig(out_png_path, bbox_inches='tight'); plt.close()

# ======= TimesNetï¼ˆKeras ç°¡åŒ–å¯¦ä½œï¼ŒTimesBlock å¤šé€±æœŸå·ç©ï¼‰=======
def _pad_to_multiple(x, period):
    """
    æŠŠé•·åº¦ L è£œåˆ°èƒ½è¢« period æ•´é™¤ï¼ˆåœ¨æ™‚é–“è»¸å°¾ç«¯è£œé›¶ï¼‰ã€‚
    å›å‚³ï¼šx_pad, L_orig(int32 Tensor), pad(int32 Tensor)
    """
    L = tf.shape(x)[1]  # åŸå§‹ Lï¼ˆTensorï¼‰
    p = tf.cast(period, tf.int32)
    rem = tf.math.mod(L, p)
    pad = tf.where(tf.equal(rem, 0), tf.zeros_like(rem), p - rem)  # è‹¥å‰›å¥½æ•´é™¤â†’pad=0
    paddings = tf.stack([[0, 0], [0, pad], [0, 0]])               # å¾Œç«¯è£œ pad
    x_pad = tf.pad(x, paddings)
    return x_pad, L, pad

class TimesBlock(layers.Layer):
    """
    TimesNet é¢¨æ ¼çš„å¤šé€±æœŸå·ç©ï¼š
      - å°å¤šå€‹ periodï¼š (B,L,C) â†’ (B, L//p, p, d) åš 2D Convï¼Œå†é‚„åŸåˆ° (B,L,d)ï¼Œå¤šé€±æœŸå¹³å‡ + æ®˜å·®
    """
    def __init__(self, d_model=128, periods=(2,3,4,5), ksize=(3,3), dropout=0.2, **kwargs):
        super().__init__(**kwargs)
        self.d_model = d_model
        self.periods = periods
        self.ksize   = ksize
        self.dropout = layers.Dropout(dropout)
        self.proj_in = layers.Dense(d_model)      # (B,L,C) â†’ (B,L,d)
        self.norm1   = layers.LayerNormalization(epsilon=1e-6)
        # æ¯å€‹ period å°æ‡‰ä¸€çµ„ Conv2D + 1x1 Conv
        self.convs   = []
        for _ in periods:
            self.convs.append(tf.keras.Sequential([
                layers.Conv2D(filters=d_model, kernel_size=ksize, padding='same', activation='relu'),
                layers.Conv2D(filters=d_model, kernel_size=1,    padding='same', activation=None),
            ]))
        self.proj_out = layers.Dense(d_model)
        self.norm2    = layers.LayerNormalization(epsilon=1e-6)

    def call(self, x, training=None):
        # x: (B, L, C)
        d_model_i32 = tf.constant(self.d_model, dtype=tf.int32)

        h = self.proj_in(x)        # (B,L,d)
        h = self.norm1(h)

        outs = []
        for p_val, conv in zip(self.periods, self.convs):
            z, L, pad = _pad_to_multiple(h, p_val)     # (B, Lp, d)
            bs = tf.shape(z)[0]
            Lp = tf.shape(z)[1]
            p  = tf.cast(p_val, tf.int32)

            # è½‰æˆ (B, Np, p, d)
            Np = tf.math.floordiv(Lp, p)
            shape1 = tf.stack([bs, Np, p, d_model_i32])
            z = tf.reshape(z, shape1)

            # 2D å·ç©
            z = conv(z, training=training)             # (B, Np, p, d)

            # é‚„åŸå› (B, Lp, d)
            shape2 = tf.stack([bs, Lp, d_model_i32])
            z = tf.reshape(z, shape2)

            # å»æ‰è£œçš„ padï¼šä¿ç•™å‰ L æ­¥ï¼ˆå…¨ Tensor sliceï¼‰
            z = tf.slice(z, begin=[0, 0, 0], size=[-1, L, -1])  # (B, L, d)

            outs.append(z)

        y = tf.add_n(outs) / float(len(outs))          # å¤šé€±æœŸå¹³å‡
        y = self.proj_out(y)
        y = self.dropout(y, training=training)

        # âœ… ä¿®æ­£æ®˜å·®ç¶­åº¦ï¼šèˆ‡ h ç›¸åŠ ï¼ˆh å·²ç‚º d_model ç¶­åº¦ï¼‰
        y = self.norm2(h + y)
        return y

def build_timesnet(input_shape,
                   d_model=128,
                   blocks=2,
                   periods_scheme="auto",
                   dropout=0.2):
    """
    ç°¡åŒ– TimesNetï¼š
      TimesBlock Ã— blocks â†’ GAP â†’ Dense(64) â†’ Dropout â†’ Dense(1,sigmoid)
    periods_scheme='auto' æœƒä¾ ws çµ¦ä¸€çµ„åˆé©çš„ periodsï¼ˆä¿è­‰å¯åˆ‡å‡º â‰¥2 å€‹ patchï¼‰ã€‚
    """
    ws, in_ch = input_shape
    if periods_scheme == "auto":
        cand = [2,3,4,5,6,7,8,10,12,15]
        periods = tuple([p for p in cand if ws // p >= 2][:4] or [2,3])
    else:
        periods = periods_scheme

    inputs = layers.Input(shape=input_shape)
    x = inputs
    for _ in range(blocks):
        x = TimesBlock(d_model=d_model, periods=periods, ksize=(3,3), dropout=dropout)(x)
    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dense(64, activation='relu')(x)
    x = layers.Dropout(dropout)(x)
    outputs = layers.Dense(1, activation='sigmoid')(x)
    model = models.Model(inputs, outputs, name='TimesNet_KerasLite')
    model._timesnet_periods = periods  # çµ¦ FLOPs ä¼°ç®—ç”¨
    model._timesnet_d_model = d_model
    model._timesnet_blocks  = blocks
    return model

# ======= FLOPs ä¼°ç®—ï¼ˆè¿‘ä¼¼ï¼Œæ¯æ¨£æœ¬ï¼‰=======
def estimate_timesnet_flops_per_sample(ws, in_ch=2, d_model=128, blocks=2, periods=(2,3,4,5), ksize=(3,3)):
    """
    è¿‘ä¼¼ 2D Conv FLOPsï¼šH*W*Cin*Kh*Kw*Cout*2
    å°æ¯å€‹ TimesBlockã€æ¯å€‹ periodï¼šæŠŠ (L,d_model) â†’ (L//p, p, d_model)ï¼Œåš Conv2D(kh,kw,d_modelâ†’d_model) + 1x1 Conv
    å¿½ç•¥ LN/Dropout/Residual ç­‰ã€‚
    """
    Kh, Kw = ksize
    flops = 0.0
    # ç¬¬ä¸€å±¤ Dense æŠ•å½± (in_châ†’d_model)ï¼š L * in_ch * d_model * 2
    flops += ws * in_ch * d_model * 2
    for _ in range(blocks):
        for p in periods:
            Np = math.ceil(ws / p)  # è£œé½Šå¾Œçš„ patch æ•¸
            # ä¸» 3x3 Conv2Dï¼š (Np * p) * d_model(in) * Kh*Kw * d_model(out) *2
            flops += (Np * p) * d_model * Kh * Kw * d_model * 2
            # 1x1 Conv2Dï¼š (Np * p) * d_model * 1 * 1 * d_model *2
            flops += (Np * p) * d_model * d_model * 2
        # block æœ«ç«¯ Dense(d_modelâ†’d_model)ï¼š L * d_model * d_model * 2
        flops += ws * d_model * d_model * 2
    # Head Dense 64 â†’ 1
    flops += d_model * 64 * 2 + 64 * 1 * 2
    return flops

# ======= ä¸»æµç¨‹ï¼ˆè¼¸å‡ºåˆ° Desktop/newtimes/times1~times4ï¼‰=======
desktop_dir = os.path.join(os.path.expanduser("~"), "Desktop")
root_out    = os.path.join(desktop_dir, "newtimes")
os.makedirs(root_out, exist_ok=True)

for ds_idx, (ds_name, csv_path) in enumerate(DATASETS, start=1):
    base_dir = os.path.join(root_out, f"times{ds_idx}")  # times1 ~ times4
    os.makedirs(base_dir, exist_ok=True)

    if not os.path.isfile(csv_path):
        print(f"âš ï¸ è³‡æ–™é›† {ds_name} è·¯å¾‘ä¸å­˜åœ¨ï¼š{csv_path}ï¼Œè·³éã€‚"); continue

    print(f"\n=== [TimesNet] è™•ç†è³‡æ–™é›† {ds_name} â†’ å„²å­˜åˆ°ï¼š{base_dir} ===")
    df = pd.read_csv(csv_path)
    if "Timestamp" in df.columns:
        df["Timestamp"] = pd.to_datetime(df["Timestamp"], errors="coerce")
        df = df.sort_values(by="Timestamp").reset_index(drop=True)
    ensure_cols(df, ["RSRP", "RSRQ", "RLF_II"])
    df[["RSRP","RSRQ","RLF_II"]] = df[["RSRP","RSRQ","RLF_II"]].fillna(0)

    rows = []

    for ws in WINDOW_SIZES:
        for fw in FUTURE_WINDOWS:
            X, y = build_xy_seq(df, ws, fw)
            if len(X) == 0:
                print(f"[TimesNet][{ds_name}] ws={ws}, fw={fw}: æ¨£æœ¬ç‚º 0ï¼Œè·³éã€‚"); continue

            idxs = np.arange(len(X))
            train_idx, val_idx = train_test_split(idxs, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y)
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]

            # æ¨™æº–åŒ–
            X_train, X_val = standardize_by_train(X_train, X_val)

            # é¡åˆ¥æ¬Šé‡
            pos = int(y_train.sum()); neg = int(len(y_train) - pos)
            class_weight = None
            if pos>0 and neg>0:
                w0 = len(y_train)/(2.0*neg); w1 = len(y_train)/(2.0*pos)
                class_weight = {0: w0, 1: w1}

            # å»ºæ¨¡ï¼ˆTimesNetï¼‰
            d_model=128; blocks=2
            model = build_timesnet(input_shape=(ws, 2),
                                   d_model=d_model, blocks=blocks,
                                   periods_scheme="auto", dropout=0.2)
            model.compile(optimizer=optimizers.Adam(1e-3),
                          loss="binary_crossentropy",
                          metrics=[metrics.AUC(name="auc")])

            cbs = [
                callbacks.EarlyStopping(monitor="val_auc", mode="max", patience=10, restore_best_weights=True),
                callbacks.ReduceLROnPlateau(monitor="val_auc", mode="max", factor=0.5, patience=5, min_lr=1e-5),
            ]

            history = model.fit(
                X_train, y_train,
                validation_data=(X_val, y_val),
                epochs=60, batch_size=128,
                class_weight=class_weight, verbose=0, callbacks=cbs
            )

            # å­¸ç¿’æ›²ç·šï¼ˆVal AUC è™›ç·šï¼‰
            lc_png = os.path.join(base_dir, f"learning_ws{ws}_fw{fw}.png")
            save_learning_curve(history, lc_png, title="TimesNet Training and Validation AUC")

            # æ¨è«–æ™‚é–“
            t0 = time.perf_counter()
            y_prob_val = model.predict(X_val, batch_size=1024, verbose=0).ravel()
            t1 = time.perf_counter()
            infer_total_s = t1 - t0
            infer_ms_per_sample = (infer_total_s / max(1, len(X_val))) * 1000.0

            # æœ€ä½³é–€æª»èˆ‡é æ¸¬
            thr, _ = best_threshold(y_val, y_prob_val)
            y_pred_val = (y_prob_val >= thr).astype(int)

            # äº‹ä»¶çŸ©é™£èˆ‡åœ–
            det_matrix, num_events_total, _ = build_event_matrices(df, ws, fw, val_idx, y_pred_val)
            out_png = os.path.join(base_dir, f"ws{ws}_fw{fw}.png")
            plot_and_save(det_matrix, ws, fw, num_events_total, out_png)

            # æŒ‡æ¨™
            acc  = accuracy_score(y_val, y_pred_val)
            try: auc = roc_auc_score(y_val, y_prob_val)
            except ValueError: auc = float("nan")
            prec = precision_score(y_val, y_pred_val, zero_division=0)
            rec  = recall_score(y_val, y_pred_val, zero_division=0)
            f1v  = f1_score(y_val, y_pred_val, zero_division=0)

            # FLOPs ä¼°ç®—
            periods = getattr(model, "_timesnet_periods", (2,3,4,5))
            flops_per_sample = estimate_timesnet_flops_per_sample(ws,
                                                                  in_ch=2, d_model=d_model,
                                                                  blocks=blocks, periods=periods, ksize=(3,3))
            flops_total_val  = flops_per_sample * len(X_val)

            print(f"[TimesNet][{ds_name}] ws={ws}, fw={fw} | thr={thr:.2f} | "
                  f"acc={acc:.3f} auc={auc:.3f} prec={prec:.3f} rec={rec:.3f} f1={f1v:.3f} | "
                  f"infer: {infer_total_s:.6f}s total, {infer_ms_per_sample:.3f} ms/sample | "
                  f"FLOPsâ‰ˆ {flops_per_sample:.0f}/sample, {flops_total_val:.0f} (val total) | "
                  f"åœ–ï¼š{out_png} / {lc_png}")

            # å­˜æ¨¡å‹
            model_path = os.path.join(base_dir, f"timesnet_ws{ws}_fw{fw}.keras")
            model.save(model_path)

            # ç´¯ç©åˆ°æ­¤è³‡æ–™é›†çš„è¡¨æ ¼
            rows.append({
                "dataset": ds_name, "ws": int(ws), "fw": int(fw),
                "thr": float(thr), "accuracy": float(acc),
                "auc": float(auc) if not np.isnan(auc) else None,
                "precision": float(prec), "recall": float(rec), "f1": float(f1v),
                "n_val": int(len(y_val)), "pos_in_val": int(y_val.sum()),
                "neg_in_val": int(len(y_val) - y_val.sum()),
                "infer_total_seconds": float(infer_total_s),
                "infer_ms_per_sample": float(infer_ms_per_sample),
                "est_FLOPs_per_sample": float(flops_per_sample),
                "est_FLOPs_val_total": float(flops_total_val),
                "periods": str(periods),
                "d_model": int(d_model), "blocks": int(blocks)
            })

    # è¼¸å‡ºæ­¤è³‡æ–™é›†çš„å½™æ•´ CSVï¼ˆè‹¥è¢«å ç”¨å‰‡ fallbackï¼‰
    if len(rows) > 0:
        df_metrics = pd.DataFrame(rows)
        csv_out = os.path.join(base_dir, "metrics_summary.csv")
        try:
            df_metrics.to_csv(csv_out, index=False, encoding="utf-8-sig")
            print(f"ğŸ“„ å·²è¼¸å‡ºæŒ‡æ¨™åˆ°ï¼š{csv_out}")
        except PermissionError:
            alt = os.path.join(base_dir, f"metrics_summary_{int(time.time())}.csv")
            df_metrics.to_csv(alt, index=False, encoding="utf-8-sig")
            print(f"âš ï¸ åŸæª”è¢«å ç”¨ï¼Œå·²æ”¹å­˜ï¼š{alt}")

print("\nâœ… å®Œæˆï¼šè«‹åˆ°æ¡Œé¢ newtimes/times1~times4 æŸ¥çœ‹ 9 å¼µäº‹ä»¶åœ– + 9 å¼µå­¸ç¿’æ›²ç·š + metrics_summary.csv + .keras æ¨¡å‹ã€‚")
